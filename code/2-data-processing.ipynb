{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from os import getcwd\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import re\n",
    "from textblob import Word, TextBlob\n",
    "from string import punctuation as pn\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim.parsing.preprocessing import STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical variable (venue)\n",
    "def encode_venues(df_train):\n",
    "    labelencoder = preprocessing.LabelEncoder()\n",
    "    encoded_labels_venue = labelencoder.fit_transform(df_train['venue'][:].tolist())\n",
    "    df_train['venues_le'] = encoded_labels_venue\n",
    "    df_train= df_train.drop([\"venue\"], axis=1)\n",
    "\n",
    "\n",
    "def process_row(row):\n",
    "   # Deleting email:\n",
    "   row = re.sub('(\\S+@\\S+)(com|\\s+com)', ' ', row)\n",
    "   # Deleting username:\n",
    "   row = re.sub('(\\S+@\\S)', ' ', row)\n",
    "   # punctuation & lower case:\n",
    "   punctuation = pn + '—“,”‘-’'\n",
    "   row = ''.join(char.lower() for char in row if char not in punctuation)\n",
    "   # Erasing stopword, converting plurals into singular, detach punctuation\n",
    "   stop = STOPWORDS\n",
    "   row = TextBlob(row)\n",
    "   row = ' '.join(Word(word).lemmatize() for word in row.words if word not in stop)\n",
    "\n",
    "   # Bring word to its root form\n",
    "   stemmer = SnowballStemmer('english')\n",
    "   row = ' '.join([stemmer.stem(word) for word in row.split() if len(word) > 2])\n",
    "   # Erase extra white space\n",
    "   row = re.sub('\\s{1,}', ' ', row)\n",
    "\n",
    "   return row\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_path():\n",
    "# Set working directory to location of the file\n",
    "   abspath = getcwd()\n",
    "   dname = os.path.dirname(abspath)\n",
    "   os.chdir(dname)\n",
    "   \n",
    "set_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paperId</th>\n",
       "      <th>title</th>\n",
       "      <th>authorId</th>\n",
       "      <th>authorName</th>\n",
       "      <th>abstract</th>\n",
       "      <th>year</th>\n",
       "      <th>venue</th>\n",
       "      <th>venues_le</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0b341b6938308a6d5f47edf490f6e46eae3835fa</td>\n",
       "      <td>detect linguist idiosyncrat interest autism di...</td>\n",
       "      <td>3188285</td>\n",
       "      <td>Masoud Rouhizadeh</td>\n",
       "      <td>child autism spectrum disord exhibit idiosyncr...</td>\n",
       "      <td>2014</td>\n",
       "      <td>CLPsych@ACL</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c682727ee058aadbe9dbf838dcb036322818f588</td>\n",
       "      <td>bigram bilstm neural network sequenti metaphor...</td>\n",
       "      <td>2782720</td>\n",
       "      <td>Yuri Bizzoni</td>\n",
       "      <td>present compar altern deep neural architectur ...</td>\n",
       "      <td>2018</td>\n",
       "      <td>Fig-Lang@NAACL-HLT</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0f9b5b32229a7245e43754430c0c88f8e7f0d8af</td>\n",
       "      <td>factual effici integr relev fact visual questi...</td>\n",
       "      <td>144748442</td>\n",
       "      <td>Peter Vickers</td>\n",
       "      <td>visual question answer vqa method aim leverag ...</td>\n",
       "      <td>2021</td>\n",
       "      <td>ACL</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    paperId  \\\n",
       "0  0b341b6938308a6d5f47edf490f6e46eae3835fa   \n",
       "1  c682727ee058aadbe9dbf838dcb036322818f588   \n",
       "2  0f9b5b32229a7245e43754430c0c88f8e7f0d8af   \n",
       "\n",
       "                                               title   authorId  \\\n",
       "0  detect linguist idiosyncrat interest autism di...    3188285   \n",
       "1  bigram bilstm neural network sequenti metaphor...    2782720   \n",
       "2  factual effici integr relev fact visual questi...  144748442   \n",
       "\n",
       "          authorName                                           abstract  year  \\\n",
       "0  Masoud Rouhizadeh  child autism spectrum disord exhibit idiosyncr...  2014   \n",
       "1       Yuri Bizzoni  present compar altern deep neural architectur ...  2018   \n",
       "2      Peter Vickers  visual question answer vqa method aim leverag ...  2021   \n",
       "\n",
       "                venue  venues_le  \n",
       "0         CLPsych@ACL         58  \n",
       "1  Fig-Lang@NAACL-HLT        122  \n",
       "2                 ACL          5  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open raw data\n",
    "df_train = pd.read_pickle(\"data/processed/dirty_df.pkl\")\n",
    "\n",
    "# Call cleaning functions\n",
    "encode_venues(df_train)\n",
    "df_train['title'] = df_train['title'].apply(process_row)\n",
    "df_train['abstract'] = df_train['abstract'].apply(process_row)\n",
    "df_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging title and abstract\n",
    "def mergingtext(df):\n",
    "   full_content = []\n",
    "   for i in range(len(df)):\n",
    "      fulltext = df.iloc[i]['title'] + ' ' + df.iloc[i]['abstract']\n",
    "      full_content.append(fulltext)\n",
    "   df['content'] = full_content\n",
    "\n",
    "mergingtext(df_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labeling author id\n",
    "auth_le = preprocessing.LabelEncoder()\n",
    "authid_enc = auth_le.fit_transform(df_train['authorId'])\n",
    "df_train['authId_enc'] = authid_enc\n",
    "df_train = df_train[['title','abstract','content', 'year', 'venues_le','authId_enc' ]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving author label to true authorId\n",
    "with open(\"code/authorIdlabel.pkl\", 'wb') as f:\n",
    "      pickle.dump(file=f, obj=auth_le)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1705\n"
     ]
    }
   ],
   "source": [
    "# spliting\n",
    "\n",
    "count = Counter(df_train['authId_enc'])\n",
    "frequentAuthor = list({k:v for k,v in count.items() if count[k] >=3}.keys())\n",
    "print(len(frequentAuthor))\n",
    "random.shuffle(frequentAuthor)\n",
    "\n",
    "val_id = []\n",
    "for auth in frequentAuthor[:500]:\n",
    "   for i in range(len(df_train)):\n",
    "      if df_train.iloc[i]['authId_enc'] == auth:\n",
    "         val_id.append(i)\n",
    "         break\n",
    "train_id = [i for i in df_train.index if i not in val_id]  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_clean_df = df_train.iloc[train_id]\n",
    "train_clean_df.reset_index(inplace=True, drop = True)\n",
    "val_clean_df = df_train.loc[val_id]\n",
    "val_clean_df.reset_index(inplace=True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_clean_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_clean_df\u001b[39m.\u001b[39mto_pickle(\u001b[39m\"\u001b[39m\u001b[39mdata/processed/train_clean_df.pkl\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m val_clean_df\u001b[39m.\u001b[39mto_pickle(\u001b[39m\"\u001b[39m\u001b[39mdata/processed/val_clean_df.pkl\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_clean_df' is not defined"
     ]
    }
   ],
   "source": [
    "train_clean_df.to_pickle(\"data/processed/train_clean_df.pkl\")\n",
    "val_clean_df.to_pickle(\"data/processed/val_clean_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open raw data\n",
    "df_test = pd.read_pickle(\"data/processed/test_dirty_df.pkl\")\n",
    "\n",
    "# Call cleaning functions\n",
    "encode_venues(df_test)\n",
    "df_test['title'] = df_test['title'].apply(process_row)\n",
    "df_test['abstract'] = df_test['abstract'].apply(process_row)\n",
    "\n",
    "mergingtext(df_test)\n",
    "\n",
    "\n",
    "# write back to processed folder\n",
    "df_test.to_pickle(\"data/processed/test_clean_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5bbd7653c429d3d2de6e8bc2b15128056d2671a20efb53a45ef350876c14518f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
